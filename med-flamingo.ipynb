{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep code and model for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "old_forward = LlamaForCausalLM.forward\n",
    "\n",
    "def forward(self, input_ids, attention_mask, **kwargs):\n",
    "    \"\"\"Condition the Flamingo layers on the media locations before forward()\"\"\"\n",
    "    if not self.initialized_flamingo:\n",
    "        raise ValueError(\n",
    "            \"Flamingo layers are not initialized. Please call `init_flamingo` first.\"\n",
    "        )\n",
    "\n",
    "    media_locations = input_ids == self.media_token_id\n",
    "\n",
    "    # if there are media already cached and we're generating and there are no media tokens in the input,\n",
    "    # we'll assume that ALL input tokens should attend to the last previous media that is cached.\n",
    "    # this is especially important for HF generate() compatibility, since generate() calls forward()\n",
    "    # repeatedly one token at a time (with no media tokens).\n",
    "    # without this check, the model would not attend to any images when generating (after the first token)\n",
    "    use_cached_media_locations = (\n",
    "        self._use_cached_vision_x\n",
    "        and self.is_conditioned()\n",
    "        and not media_locations.any()\n",
    "    )\n",
    "\n",
    "    for layer in self._get_decoder_layers():\n",
    "        if not use_cached_media_locations:\n",
    "            layer.condition_media_locations(media_locations)\n",
    "        layer.condition_use_cached_media(use_cached_media_locations)\n",
    "\n",
    "    # package arguments for the other parent's forward. since we don't know the order of the arguments,\n",
    "    # make them all kwargs\n",
    "    kwargs[\"input_ids\"] = input_ids\n",
    "    kwargs[\"attention_mask\"] = attention_mask\n",
    "    return old_forward(self, **kwargs)  # Call the other parent's forward method\n",
    "\n",
    "\n",
    "\n",
    "LlamaForCausalLM.forward = forward\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import open_clip\n",
    "\n",
    "from open_flamingo.src.flamingo import Flamingo\n",
    "from open_flamingo.src.flamingo_lm import FlamingoLMMixin\n",
    "from open_flamingo.src.utils import extend_instance\n",
    "\n",
    "def create_model_and_transforms(\n",
    "    clip_vision_encoder_path: str,\n",
    "    clip_vision_encoder_pretrained: str,\n",
    "    lang_encoder_path: str,\n",
    "    tokenizer_path: str,\n",
    "    cross_attn_every_n_layers: int = 1,\n",
    "    use_local_files: bool = False,\n",
    "    decoder_layers_attr_name: str = None,\n",
    "    freeze_lm_embeddings: bool = False,\n",
    "    **flamingo_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize a Flamingo model from a pretrained vision encoder and language encoder.\n",
    "    Appends special tokens to the tokenizer and freezes backbones.\n",
    "\n",
    "    Args:\n",
    "        clip_vision_encoder_path (str): path to pretrained clip model (e.g. \"ViT-B-32\")\n",
    "        clip_vision_encoder_pretrained (str): name of pretraining dataset for clip model (e.g. \"laion2b_s32b_b79k\")\n",
    "        lang_encoder_path (str): path to pretrained language encoder\n",
    "        tokenizer_path (str): path to pretrained tokenizer\n",
    "        cross_attn_every_n_layers (int, optional): determines how often to add a cross-attention layer. Defaults to 1.\n",
    "        use_local_files (bool, optional): whether to use local files. Defaults to False.\n",
    "        decoder_layers_attr_name (str, optional): name of the decoder layers attribute. Defaults to None.\n",
    "    Returns:\n",
    "        Flamingo: Flamingo model from pretrained vision and language encoders\n",
    "        Image processor: Pipeline to preprocess input images\n",
    "        Tokenizer: A tokenizer for the language model\n",
    "    \"\"\"\n",
    "    vision_encoder, _, image_processor = open_clip.create_model_and_transforms(\n",
    "        clip_vision_encoder_path, pretrained=clip_vision_encoder_pretrained\n",
    "    )\n",
    "    # set the vision encoder to output the visual features\n",
    "    vision_encoder.visual.output_tokens = True\n",
    "\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_path,\n",
    "        local_files_only=use_local_files,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    # add Flamingo special tokens to the tokenizer\n",
    "    text_tokenizer.add_special_tokens(\n",
    "        {\"additional_special_tokens\": [\"<|endofchunk|>\", \"<image>\"]}\n",
    "    )\n",
    "    if text_tokenizer.pad_token is None:\n",
    "        # Issue: GPT models don't have a pad token, which we use to\n",
    "        # modify labels for the loss.\n",
    "        text_tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "\n",
    "    lang_encoder = AutoModelForCausalLM.from_pretrained(\n",
    "        lang_encoder_path,\n",
    "        local_files_only=use_local_files,\n",
    "        trust_remote_code=True,\n",
    "        load_in_4bit = True\n",
    "    )\n",
    "\n",
    "    # convert LM to FlamingoLM\n",
    "    extend_instance(lang_encoder, FlamingoLMMixin)\n",
    "\n",
    "    if decoder_layers_attr_name is None:\n",
    "        decoder_layers_attr_name = _infer_decoder_layers_attr_name(lang_encoder)\n",
    "    lang_encoder.set_decoder_layers_attr_name(decoder_layers_attr_name)\n",
    "    lang_encoder.resize_token_embeddings(len(text_tokenizer))\n",
    "\n",
    "    model = Flamingo(\n",
    "        vision_encoder,\n",
    "        lang_encoder,\n",
    "        text_tokenizer.encode(\"<|endofchunk|>\")[-1],\n",
    "        text_tokenizer.encode(\"<image>\")[-1],\n",
    "        vis_dim=open_clip.get_model_config(clip_vision_encoder_path)[\"vision_cfg\"][\n",
    "            \"width\"\n",
    "        ],\n",
    "        cross_attn_every_n_layers=cross_attn_every_n_layers,\n",
    "        **flamingo_kwargs,\n",
    "    )\n",
    "\n",
    "    # Freeze all parameters\n",
    "    model.requires_grad_(False)\n",
    "    assert sum(p.numel() for p in model.parameters() if p.requires_grad) == 0\n",
    "\n",
    "    # Unfreeze perceiver, gated_cross_attn_layers, and LM input embeddings\n",
    "    model.perceiver.requires_grad_(True)\n",
    "    model.lang_encoder.gated_cross_attn_layers.requires_grad_(True)\n",
    "    if not freeze_lm_embeddings:\n",
    "        model.lang_encoder.get_input_embeddings().requires_grad_(True)\n",
    "        # TODO: investigate also training the output embeddings when untied\n",
    "\n",
    "    print(\n",
    "        f\"Flamingo model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\"\n",
    "    )\n",
    "\n",
    "    return model, image_processor, text_tokenizer\n",
    "\n",
    "\n",
    "def _infer_decoder_layers_attr_name(model):\n",
    "    for k in __KNOWN_DECODER_LAYERS_ATTR_NAMES:\n",
    "        if k.lower() in model.__class__.__name__.lower():\n",
    "            return __KNOWN_DECODER_LAYERS_ATTR_NAMES[k]\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.\"\n",
    "    )\n",
    "\n",
    "\n",
    "__KNOWN_DECODER_LAYERS_ATTR_NAMES = {\n",
    "    \"opt\": \"model.decoder.layers\",\n",
    "    \"gptj\": \"transformer.h\",\n",
    "    \"gpt-j\": \"transformer.h\",\n",
    "    \"pythia\": \"gpt_neox.layers\",\n",
    "    \"llama\": \"model.layers\",\n",
    "    \"gptneoxforcausallm\": \"gpt_neox.layers\",\n",
    "    \"mpt\": \"transformer.blocks\",\n",
    "    \"mosaicgpt\": \"transformer.blocks\",\n",
    "}\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "checkpoint_path = hf_hub_download(\"med-flamingo/med-flamingo\", \"model.pt\")\n",
    "print(f'Downloaded Med-Flamingo checkpoint to {checkpoint_path}')\n",
    "\n",
    "import torch\n",
    "\n",
    "a = torch.load(checkpoint_path, map_location=\"cuda\")\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "import os\n",
    "#from open_flamingo import create_model_and_transforms\n",
    "from accelerate import Accelerator\n",
    "from einops import repeat\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "from src.utils import FlamingoProcessor\n",
    "\n",
    "def clean_generation(response):\n",
    "    \"\"\"\n",
    "    for some reason, the open-flamingo based model slightly changes the input prompt (e.g. prepends <unk>, an adds some spaces)\n",
    "    \"\"\"\n",
    "    return response.replace('<unk> ', '').strip()\n",
    "\n",
    "accelerator = Accelerator() #when using cpu: cpu=True\n",
    "\n",
    "device = accelerator.device\n",
    "\n",
    "print('Loading model..')\n",
    "\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"huggyllama/llama-7b\",\n",
    "    tokenizer_path= \"huggyllama/llama-7b\",\n",
    "    cross_attn_every_n_layers=4,\n",
    ")\n",
    "\n",
    "model.perceiver.cuda()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.vision_encoder.cuda()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.lang_encoder.gated_cross_attn_layers.to(torch.float16).cuda()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import gc  # Import the garbage collection module\n",
    "\n",
    "# Collect and free up unused memory\n",
    "gc.collect()\n",
    "\n",
    "# load med-flamingo checkpoint:\n",
    "model.load_state_dict(a, strict=False)\n",
    "processor = FlamingoProcessor(tokenizer, image_processor)\n",
    "\n",
    "# Prepare the model using the Accelerator\n",
    "model = accelerator.prepare(model)\n",
    "is_main_process = accelerator.is_main_process\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation code. Adjust as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images into PIL list\n",
    "image_paths = [\n",
    "    \"img/brokenarm.jpg\",\n",
    "    \"img/normalleg.jpg\",\n",
    "    \"img/brokentoe.jpg\",\n",
    "    \"img/brokenwrist.jpg\",\n",
    "    \"img/normalarm.jpg\",\n",
    "]\n",
    "demo_images = [Image.open(path) for path in image_paths]\n",
    "\n",
    "# Define a few-shot prompt containing text and <image> placeholders\n",
    "prompt = \"You are a helpful medical assistant. You are being provided with images,\" \\\n",
    "    \"a question about the image and an answer. Follow the examples and answer the last question.\" \\\n",
    "    \"<image>Question: What is the report? Answer: fracture.<|endofchunk|>\" \\\n",
    "    \"<image>Question: What is the report? Answer: no fracture.<|endofchunk|>\" \\\n",
    "    \"<image>Question: What is the report? Answer: fracture.<|endofchunk|>\" \\\n",
    "    \"<image>Question: What is the report? Answer: fracture.<|endofchunk|>\" \\\n",
    "    \"<image>Question: What is the report? Answer:\"\n",
    "\n",
    "# Preprocess demo images using the FlamingoProcessor\n",
    "pixels = processor.preprocess_images(demo_images)\n",
    "\n",
    "pixels = repeat(pixels, 'N c h w -> b N T c h w', b=1, T=1)\n",
    "\n",
    "# Encode the text prompt using the FlamingoProcessor\n",
    "tokenized_data = processor.encode_text(prompt)\n",
    "\n",
    "# Generate response\n",
    "# Use mixed-precision training context for improved performance\n",
    "with torch.autocast('cuda', torch.float16):\n",
    "\n",
    "    # Generate text using the model\n",
    "    generated_text = model.generate(\n",
    "        vision_x=pixels.to(device),  # Convert images to the device\n",
    "        lang_x=tokenized_data[\"input_ids\"].to(device),  # Convert text input to the device\n",
    "        attention_mask=tokenized_data[\"attention_mask\"].to(device),  # Convert attention mask to the device\n",
    "        max_new_tokens=20,  # Limit the maximum number of new tokens in the generated response\n",
    "    )\n",
    "\n",
    "# Decode the generated text using the processor's tokenizer\n",
    "response = processor.tokenizer.decode(generated_text[0])\n",
    "\n",
    "# Clean up the generated response\n",
    "response = clean_generation(response)\n",
    "\n",
    "# Print the cleaned response\n",
    "print(response.split(\"Question:\")[-1].split(\".\")[0].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
